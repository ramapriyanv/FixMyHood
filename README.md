# FixMyHood ðŸ™ï¸

A full-stack civic issue reporting app with an integrated AI-powered assistant. Users can report problems in their neighborhood â€” such as potholes, broken streetlights, or trash dumps â€” through a simple chat-based interface.

---

## ðŸš€ Features

- ðŸ’¬ **AI Assistant Interface**: Chat-style form that guides users through issue reporting.
- ðŸ“· **Image Uploads**: Attach images with each issue report.
- ðŸŒ— **Dark/Light Mode**: Toggle between themes with a single click.
- ðŸŒ **React + Flask**: Frontend built with React, backend powered by Flask + SQLite.

---

## ðŸ“ Project Structure

```
FixMyHood/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ uploads/
â”‚   â””â”€â”€ frontend/ (production React build)
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ ChatAssistant.js
â”‚   â”‚   â”œâ”€â”€ ViewReports.js
â”‚   â”‚   â”œâ”€â”€ Login.js
â”‚   â”‚   â”œâ”€â”€ Register.js
â”‚   â”‚   â””â”€â”€ App.js
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ðŸ› ï¸ Tech Stack

| Frontend        | Backend        | Database  | AI Assistant   |
|-----------------|----------------|-----------|----------------|
| React           | Flask          | SQLite    | Local AI model |

---

## ðŸ§ª Local Setup Instructions

### 1. Clone the repo

```bash
git clone https://github.com/ramapriyanv/FixMyHood.git
cd FixMyHood
```

### 2. Backend Setup (Python 3.11+)

Install dependencies:

```bash
pip install -r requirements.txt
```

Run Flask backend:

```bash
cd backend
python app.py
```

### 3. Frontend Setup

```bash
cd frontend
npm install
npm run build
```

Then copy the build:

```bash
xcopy /E /I /Y build ..\backend\frontend\build
```

---

## ðŸ§  AI Assistant (Powered by Ollama)

This project uses a **local LLM** for the AI assistant via [Ollama](https://ollama.com/).

### Steps to enable the assistant:

1. **Download and install Ollama**  
   ðŸ‘‰ https://ollama.com/download

2. **Start Ollama and run a model locally**  
   For example:

   ```bash
   ollama run mistral
   ```

3. **Make sure it runs at http://localhost:11434** (default)

4. Youâ€™re all set! The assistant in the React app will now communicate with this local AI model.

> ðŸ’¡ You can also swap out the model (like `llama3`, `phi3`, etc.) depending on your needs.

---

## ðŸ“œ License

MIT License â€” free to use, modify, and distribute.

